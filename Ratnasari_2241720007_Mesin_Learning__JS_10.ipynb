{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ratnasari124/Mechine_Learning_Ratnasari/blob/main/Ratnasari_2241720007_Mesin_Learning__JS_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Job Sheet 10**\n",
        "\n",
        "Recurrent Neural Network (RNN)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Nama : Ratnasari (2241720007)\n",
        "\n",
        "\n",
        "---\n",
        "Link Jobsheet : https://polinema.gitbook.io/jti-modul-praktikum-pembelajaran-mesin/job-sheet-10-recurrent-neural-network-rnn\n",
        "\n",
        "Link Github :"
      ],
      "metadata": {
        "id": "eanmgVHYwpYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PRAKTIKUM 1**\n",
        "\n",
        "---\n",
        "\n",
        "RNN untuk Analisis Sentimen"
      ],
      "metadata": {
        "id": "nDA6UTEpw8kD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e4QJFz18vIrP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ],
      "metadata": {
        "id": "aEx16Xp2yMbp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup input pipeline\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "37MqX_aByhBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset menggunakan TFDS. Lihat loading text tutorial jika ingin me load data secara manua"
      ],
      "metadata": {
        "id": "v7vB6vE30eGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
        "                          as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhWwQ8WCzTeF",
        "outputId": "00065590-5907-4750-8f9d-456a0c7383bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n",
            "Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awalnya ini mengembalikan dataset (teks, pasangan label):"
      ],
      "metadata": {
        "id": "Eh3w8fFm0bAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('text: ', example.numpy())\n",
        "  print('label: ', label.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "484Nzfwtz2xq",
        "outputId": "a76dae33-cf73-46e7-cfd0-86b7b405f568"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "label:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berikutnya acak data untuk pelatihan dan membuat kumpulan pasangan (teks, label) ini:"
      ],
      "metadata": {
        "id": "h9T2V9Xoz-yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4bCxzwnz5ON",
        "outputId": "b0e96856-eb93-4263-8bcf-74ad52ca2c3b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texts:  [b'Some people seem to think this was the worst movie they have ever seen, and I understand where they\\'re coming from, but I really have seen worse.<br /><br />That being said, the movies that I can recall (ie the ones I haven\\'t blocked out) that were worse than this, were so bad that they physically pained every sense that was involved with watching the movie. The movies that are worse than War Games 2 are the ones that make you want to gouge out your eyes, or stab sharp objects in your ears to keep yourself from having another piece of your soul ripped away from you by the awfulness.<br /><br />War Games: The Dead Code isn\\'t that bad, but it comes pretty close. Yes I was a fan of the original, but no I wasn\\'t expecting miracles from this one. Let\\'s face it the original wasn\\'t really that great of a movie in the first place, it was basically just a campy 80s teen romance flick with some geek-appeal to it.<br /><br />That\\'s all I was hoping for, something bad, but that might have tugged at my geek-strings. Was that too much to ask for? Is it really not possible to do better than the original War Games, even for a straight to video release? Well apparently that was too much to ask for. Stay away from this movie. At first it\\'s just bad, like \"Oh yeah, this is bad, but I\\'m kind of enjoying it, maybe the end will be good like in the original.\" And then it just gets worse and worse, and by the end, trust me, you will wish you had not seen this movie.'\n",
            " b\"I'm a Black man living in a predominantly Black city. That being said, I have some major misgivings about Tyler Perry's work. I realize that some people out there feel the need to praise him, because he's Black and trying to portray a positive image about the culture. But, I honestly do believe that, were Perry White, this film would have had the NAACP, Al Sharpton, and Jessie Jackson all over his ass.<br /><br />I have been forced to watch this movie one whole hell of a lot recently and each repeated viewing makes my blood boil. The characters are poorly written and acted. The jokes are so bad, I have to actually be told something is supposed to be funny. I'm just going to break this big pile of sh-t down.<br /><br />Madea=suck. The character may have had some appeal, but it doesn't anymore. When the only thing she ever seems to do is smack around children and threaten adults with violence she is less than useless. She is unnecessary.<br /><br />The situation with the wife beating fianc\\xc3\\xa9 was horsesh-t. If a woman was so scared to death of her husband, why would she try to run away when he's sleeping in bed. Wouldn't it have made more sense for her to leave when he was at work. At any rate, the characters in this arc were so annoying and overbearing that I hoped he would throw her off the balcony and was royally ticked when he didn't.<br /><br />Then there are the two lovebirds. A bus driver asks a woman out by harassing her while he's making his rounds. I couldn't believe it. I really couldn't believe when she agreed to go out with him even more. But, what takes the cake is that a grown man was reduced to tossing pebbles at a window and passing notes like a ten year old by a castrating mega bitch. I don't use this term lightly, but that woman only had two modes. Morose victim and psycho momma. No matter which of these two faces she showed, however, there was one constant. The bus driver wasn't going to get any. He even married her without sampling the goods--WTF! <br /><br />Then there's the family reunion scene. Here we've got the mother load which includes implied incestual taboos, grinding for the sake of grinding, shirtless, overly musclebound, b-ball, plus the great taste of Maya Angelou. When those babes dragged their butts outside and called a meeting, was I wrong to wish that the oldest of them was claimed by a heart attack. All this crap is going on at the reunion, in laughably easy to separate groups, and then they ring a bell. When they do, everyone drops what they're doing and heads on over for a stern talking too, just like a pack of Pavlov's doggies--WTF!! <br /><br />Then you have the final five minutes of the film. In it we see the abusive fianc\\xc3\\xa9 get manhandled by his longtime victim and all around bad actress. There is an impromptu wedding where Black people are dressed like angels and are hanging from the ceiling--WTF!!! The only reason to watch it this far, besides testing your threshold for pain, is the hope that the second villain of this story gets her ass handed to her as well. Guess what, it doesn't happen. Instead, Perry takes the testicularly challenged way out and plays it safe, ending the movie on a tone of forgiveness--WTF!! <br /><br />I'm pretty sure that, if given a day , I could probably write a doctoral dissertation on all the ways this movie sucks. Don't even get me started on the rest of Tyler Perry's films. I'm just going to say this. In my opinion, as a Black guy, D.W. Griffith's legacy lives on. The irony is that it is doing so through a Black man who will be praised for doing what Birth of a Nation did, selling us down the river. I only wish Perry's films were dudes so I could kick them in the nuts. Thanks a lot, dude!! What are you going to follow this up with in 2009, a comedy about the raping and savage beating of slaves in Colonial America?\"\n",
            " b'This is sad this movie is the tops this should at least be in the top 250 movies here. This is still the best Action movie ever done. The action movies of today are badly done The actors and action directors do not no how to do it fighting and stunts properly. only some no how to do it mostly from Hong Kong like Jackie Chan. The stunts are so clever and wild i do not think we will see the likes of ever. The start where Chan and his team go down the hill car chase through the hill town is just amazing. The end fight stunts are for me the best fight stunts ever put to film. The end stunt sliding down the pole crashing through the glass Jackie was badly hurt.']\n",
            "\n",
            "labels:  [0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buat Teks Encoder\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uqkGN6dh0ird"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 1000\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))"
      ],
      "metadata": {
        "id": "f1gvOXqZ0j_L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab=np.array(encoder.get_vocabulary())\n",
        "vocab[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT6-bZIj0m63",
        "outputId": "ebb9c128-ac07-46dd-b9f6-122b52424d15"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
              "      dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_example=encoder(example)[:3].numpy()\n",
        "encoded_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mDeeoBHTXnq",
        "outputId": "ce0dc161-5b94-4227-db92-3e87ed142850"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 47,  83, 289, ...,   0,   0,   0],\n",
              "       [142,   4, 323, ...,   0,   0,   0],\n",
              "       [ 11,   7, 614, ...,   0,   0,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKCKRPeXTpTx",
        "outputId": "7356fa66-5ede-4b6d-87d1-3b1d3df86c13"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  b'Some people seem to think this was the worst movie they have ever seen, and I understand where they\\'re coming from, but I really have seen worse.<br /><br />That being said, the movies that I can recall (ie the ones I haven\\'t blocked out) that were worse than this, were so bad that they physically pained every sense that was involved with watching the movie. The movies that are worse than War Games 2 are the ones that make you want to gouge out your eyes, or stab sharp objects in your ears to keep yourself from having another piece of your soul ripped away from you by the awfulness.<br /><br />War Games: The Dead Code isn\\'t that bad, but it comes pretty close. Yes I was a fan of the original, but no I wasn\\'t expecting miracles from this one. Let\\'s face it the original wasn\\'t really that great of a movie in the first place, it was basically just a campy 80s teen romance flick with some geek-appeal to it.<br /><br />That\\'s all I was hoping for, something bad, but that might have tugged at my geek-strings. Was that too much to ask for? Is it really not possible to do better than the original War Games, even for a straight to video release? Well apparently that was too much to ask for. Stay away from this movie. At first it\\'s just bad, like \"Oh yeah, this is bad, but I\\'m kind of enjoying it, maybe the end will be good like in the original.\" And then it just gets worse and worse, and by the end, trust me, you will wish you had not seen this movie.'\n",
            "Round-trip:  some people seem to think this was the worst movie they have ever seen and i understand where theyre coming from but i really have seen [UNK] br that being said the movies that i can [UNK] [UNK] the ones i havent [UNK] out that were worse than this were so bad that they [UNK] [UNK] every sense that was involved with watching the movie the movies that are worse than war [UNK] 2 are the ones that make you want to [UNK] out your eyes or [UNK] [UNK] [UNK] in your [UNK] to keep yourself from having another piece of your [UNK] [UNK] away from you by the [UNK] br war [UNK] the dead [UNK] isnt that bad but it comes pretty close yes i was a fan of the original but no i wasnt expecting [UNK] from this one lets face it the original wasnt really that great of a movie in the first place it was basically just a [UNK] 80s [UNK] romance flick with some [UNK] to itbr br thats all i was [UNK] for something bad but that might have [UNK] at my [UNK] was that too much to ask for is it really not possible to do better than the original war [UNK] even for a straight to video release well apparently that was too much to ask for stay away from this movie at first its just bad like oh [UNK] this is bad but im kind of [UNK] it maybe the end will be good like in the original and then it just gets worse and worse and by the end [UNK] me you will wish you had not seen this movie                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
            "\n",
            "Original:  b\"I'm a Black man living in a predominantly Black city. That being said, I have some major misgivings about Tyler Perry's work. I realize that some people out there feel the need to praise him, because he's Black and trying to portray a positive image about the culture. But, I honestly do believe that, were Perry White, this film would have had the NAACP, Al Sharpton, and Jessie Jackson all over his ass.<br /><br />I have been forced to watch this movie one whole hell of a lot recently and each repeated viewing makes my blood boil. The characters are poorly written and acted. The jokes are so bad, I have to actually be told something is supposed to be funny. I'm just going to break this big pile of sh-t down.<br /><br />Madea=suck. The character may have had some appeal, but it doesn't anymore. When the only thing she ever seems to do is smack around children and threaten adults with violence she is less than useless. She is unnecessary.<br /><br />The situation with the wife beating fianc\\xc3\\xa9 was horsesh-t. If a woman was so scared to death of her husband, why would she try to run away when he's sleeping in bed. Wouldn't it have made more sense for her to leave when he was at work. At any rate, the characters in this arc were so annoying and overbearing that I hoped he would throw her off the balcony and was royally ticked when he didn't.<br /><br />Then there are the two lovebirds. A bus driver asks a woman out by harassing her while he's making his rounds. I couldn't believe it. I really couldn't believe when she agreed to go out with him even more. But, what takes the cake is that a grown man was reduced to tossing pebbles at a window and passing notes like a ten year old by a castrating mega bitch. I don't use this term lightly, but that woman only had two modes. Morose victim and psycho momma. No matter which of these two faces she showed, however, there was one constant. The bus driver wasn't going to get any. He even married her without sampling the goods--WTF! <br /><br />Then there's the family reunion scene. Here we've got the mother load which includes implied incestual taboos, grinding for the sake of grinding, shirtless, overly musclebound, b-ball, plus the great taste of Maya Angelou. When those babes dragged their butts outside and called a meeting, was I wrong to wish that the oldest of them was claimed by a heart attack. All this crap is going on at the reunion, in laughably easy to separate groups, and then they ring a bell. When they do, everyone drops what they're doing and heads on over for a stern talking too, just like a pack of Pavlov's doggies--WTF!! <br /><br />Then you have the final five minutes of the film. In it we see the abusive fianc\\xc3\\xa9 get manhandled by his longtime victim and all around bad actress. There is an impromptu wedding where Black people are dressed like angels and are hanging from the ceiling--WTF!!! The only reason to watch it this far, besides testing your threshold for pain, is the hope that the second villain of this story gets her ass handed to her as well. Guess what, it doesn't happen. Instead, Perry takes the testicularly challenged way out and plays it safe, ending the movie on a tone of forgiveness--WTF!! <br /><br />I'm pretty sure that, if given a day , I could probably write a doctoral dissertation on all the ways this movie sucks. Don't even get me started on the rest of Tyler Perry's films. I'm just going to say this. In my opinion, as a Black guy, D.W. Griffith's legacy lives on. The irony is that it is doing so through a Black man who will be praised for doing what Birth of a Nation did, selling us down the river. I only wish Perry's films were dudes so I could kick them in the nuts. Thanks a lot, dude!! What are you going to follow this up with in 2009, a comedy about the raping and savage beating of slaves in Colonial America?\"\n",
            "Round-trip:  im a black man living in a [UNK] black city that being said i have some major [UNK] about [UNK] [UNK] work i realize that some people out there feel the need to [UNK] him because hes black and trying to [UNK] a [UNK] [UNK] about the [UNK] but i [UNK] do believe that were [UNK] white this film would have had the [UNK] [UNK] [UNK] and [UNK] [UNK] all over his [UNK] br i have been forced to watch this movie one whole hell of a lot [UNK] and each [UNK] viewing makes my blood [UNK] the characters are poorly written and [UNK] the jokes are so bad i have to actually be told something is supposed to be funny im just going to [UNK] this big [UNK] of [UNK] [UNK] br [UNK] the character may have had some [UNK] but it doesnt [UNK] when the only thing she ever seems to do is [UNK] around children and [UNK] [UNK] with violence she is less than [UNK] she is [UNK] br the situation with the wife [UNK] [UNK] was [UNK] if a woman was so [UNK] to death of her husband why would she try to run away when hes [UNK] in [UNK] wouldnt it have made more sense for her to leave when he was at work at any [UNK] the characters in this [UNK] were so annoying and [UNK] that i [UNK] he would [UNK] her off the [UNK] and was [UNK] [UNK] when he [UNK] br then there are the two [UNK] a [UNK] [UNK] [UNK] a woman out by [UNK] her while hes making his [UNK] i couldnt believe it i really couldnt believe when she [UNK] to go out with him even more but what takes the [UNK] is that a [UNK] man was [UNK] to [UNK] [UNK] at a [UNK] and [UNK] [UNK] like a ten year old by a [UNK] [UNK] [UNK] i dont use this [UNK] [UNK] but that woman only had two [UNK] [UNK] [UNK] and [UNK] [UNK] no matter which of these two [UNK] she [UNK] however there was one [UNK] the [UNK] [UNK] wasnt going to get any he even [UNK] her without [UNK] the [UNK] br br then theres the family [UNK] scene here [UNK] got the mother [UNK] which [UNK] [UNK] [UNK] [UNK] [UNK] for the [UNK] of [UNK] [UNK] [UNK] [UNK] [UNK] plus the great [UNK] of [UNK] [UNK] when those [UNK] [UNK] their [UNK] outside and called a [UNK] was i wrong to wish that the [UNK] of them was [UNK] by a heart [UNK] all this crap is going on at the [UNK] in [UNK] easy to [UNK] [UNK] and then they [UNK] a [UNK] when they do everyone [UNK] what theyre doing and [UNK] on over for a [UNK] talking too just like a [UNK] of [UNK] [UNK] br br then you have the final five minutes of the film in it we see the [UNK] [UNK] get [UNK] by his [UNK] [UNK] and all around bad actress there is an [UNK] [UNK] where black people are [UNK] like [UNK] and are [UNK] from the [UNK] the only reason to watch it this far [UNK] [UNK] your [UNK] for [UNK] is the hope that the second [UNK] of this story gets her [UNK] [UNK] to her as well guess what it doesnt happen instead [UNK] takes the [UNK] [UNK] way out and plays it [UNK] ending the movie on a [UNK] of [UNK] br br im pretty sure that if given a day i could probably write a [UNK] [UNK] on all the ways this movie [UNK] dont even get me started on the rest of [UNK] [UNK] films im just going to say this in my opinion as a black guy [UNK] [UNK] [UNK] lives on the [UNK] is that it is doing so through a black man who will be [UNK] for doing what [UNK] of a [UNK] did [UNK] us down the [UNK] i only wish [UNK] films were [UNK] so i could [UNK] them in the [UNK] [UNK] a lot [UNK] what are you going to follow this up with in [UNK] a comedy about the [UNK] and [UNK] [UNK] of [UNK] in [UNK] america                                                                                                                                                                           \n",
            "\n",
            "Original:  b'This is sad this movie is the tops this should at least be in the top 250 movies here. This is still the best Action movie ever done. The action movies of today are badly done The actors and action directors do not no how to do it fighting and stunts properly. only some no how to do it mostly from Hong Kong like Jackie Chan. The stunts are so clever and wild i do not think we will see the likes of ever. The start where Chan and his team go down the hill car chase through the hill town is just amazing. The end fight stunts are for me the best fight stunts ever put to film. The end stunt sliding down the pole crashing through the glass Jackie was badly hurt.'\n",
            "Round-trip:  this is sad this movie is the [UNK] this should at least be in the top [UNK] movies here this is still the best action movie ever done the action movies of today are badly done the actors and action directors do not no how to do it fighting and [UNK] [UNK] only some no how to do it mostly from [UNK] [UNK] like [UNK] [UNK] the [UNK] are so [UNK] and [UNK] i do not think we will see the [UNK] of ever the start where [UNK] and his team go down the [UNK] car [UNK] through the [UNK] town is just amazing the end fight [UNK] are for me the best fight [UNK] ever put to film the end [UNK] [UNK] down the [UNK] [UNK] through the [UNK] [UNK] was badly [UNK]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buat Model\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_X_jxksDTsRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "ciGnbUnOT_A_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([layer.supports_masking for layer in model.layers])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HpfBCeDUFCj",
        "outputId": "89718580-4a92-494a-bfec-8231d60d12d3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False, True, True, True, True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset, validation_steps=30)\n",
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "sample_text_sensor =tf.constant([sample_text])\n",
        "encoded_text = encoder(sample_text_sensor)\n",
        "predictions = model.predict(encoded_text)\n",
        "print(predictions[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSIK2MkgUXU9",
        "outputId": "ef5c3529-8b40-45f3-a4d1-34e64766a7f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m663s\u001b[0m 2s/step - accuracy: 0.5179 - loss: 0.6813 - val_accuracy: 0.7547 - val_loss: 0.5813\n",
            "Epoch 2/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m661s\u001b[0m 2s/step - accuracy: 0.8048 - loss: 0.4342 - val_accuracy: 0.8448 - val_loss: 0.3606\n",
            "Epoch 3/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m669s\u001b[0m 2s/step - accuracy: 0.8477 - loss: 0.3502 - val_accuracy: 0.8771 - val_loss: 0.3332\n",
            "Epoch 4/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m671s\u001b[0m 2s/step - accuracy: 0.8629 - loss: 0.3198 - val_accuracy: 0.8490 - val_loss: 0.3545\n",
            "Epoch 5/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 2s/step - accuracy: 0.8634 - loss: 0.3161 - val_accuracy: 0.8375 - val_loss: 0.3311\n",
            "Epoch 6/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 2s/step - accuracy: 0.8656 - loss: 0.3122 - val_accuracy: 0.8354 - val_loss: 0.3452\n",
            "Epoch 7/10\n",
            "\u001b[1m197/391\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5:20\u001b[0m 2s/step - accuracy: 0.8685 - loss: 0.3044"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predict on a sample text with padding\n",
        "\n",
        "padding = \"the\" * 2000\n",
        "predictions = model.predict(np.array([sample_text, padding]))\n",
        "print(predictions[0])"
      ],
      "metadata": {
        "id": "hGGy0tlpWLek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#** PRAKTIKUM 2**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Generator Teks dengan RNN\n"
      ],
      "metadata": {
        "id": "IrenI5lmxEKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup**"
      ],
      "metadata": {
        "id": "t61RzkWq00za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import TensorFlow"
      ],
      "metadata": {
        "id": "zailPcdG09Mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "-F5bKYTDxREV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file(\n",
        "    'shakespeare.txt',  # This should be the filename you want to save it as\n",
        "    origin='https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'  # This is the URL of the file\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GREr1nNz1XXC",
        "outputId": "319e8efd-e5d5-4b10-adc2-72df96f7812f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "M-0RLeXN12wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dm7AwhI14cQ",
        "outputId": "abdc61eb-23f5-4da2-d9e0-6b5e35a96945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P3yD6bl16ru",
        "outputId": "aca33485-13e7-4731-99cb-722b69885dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONWEQ69g1-TL",
        "outputId": "48379eb4-d310-4d7f-b285-3a241792e18c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Olah Teks**"
      ],
      "metadata": {
        "id": "sYcO2h112CP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize Teks\n"
      ],
      "metadata": {
        "id": "ldvAm85A2FhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts=['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ2CWFh52J6I",
        "outputId": "2e7bcc40-edff-45da-c191-1d5dad7519bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "uNn2U4Jl3Nog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwrVFAoQ3Zjk",
        "outputId": "94e205d1-f3fc-44b7-98ee-cda0dcfa9fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "e0Hzd5zJ3pb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3YN9FP53sKh",
        "outputId": "c054ce0b-07ab-45f7-8d44-1f7bf8e6ea81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIrKLqWf3yHk",
        "outputId": "e1b8a858-8c5f-4268-bdcf-f6ed8aed6429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "yf35oK_H34CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_kmjuyU4A3g",
        "outputId": "edca157b-aa89-4681-9310-ab4918aca2f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "MpunGQQZ4EcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm29uL024Hbf",
        "outputId": "d9efa5a4-caa0-4d58-8aa7-483d4f5ed1d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "CvKyejBN4Moz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM-rkbbx4O6S",
        "outputId": "e589884f-2f5d-4753-9dc6-0b7efe6d372f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cth2MBg4SBD",
        "outputId": "187c3179-84a7-4634-e6fe-2a49f52b0cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "DW_h0szF4WF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOdD8LNj4XHQ",
        "outputId": "8d74911f-6527-4340-9b86-1fe9cdd01c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "Jprz49GW4ad0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umKB0CJ34eeK",
        "outputId": "5e7119cd-2f93-4933-f9cb-aa66c1a27cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Membuat Batch Training**"
      ],
      "metadata": {
        "id": "ojo0kPQ64i3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AvzKk_P4o9z",
        "outputId": "3a86f0db-47f6-4d44-e9a4-d99039bba816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Buat Model**"
      ],
      "metadata": {
        "id": "embYuc_j6YUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "iLhhX8J06dAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      # Fix: Initialize the state with the correct shape using tf.zeros\n",
        "      batch_size = tf.shape(x)[0]\n",
        "      states = tf.zeros([batch_size, self.gru.units], dtype=tf.float32) # Initialize with zeros and correct shape\n",
        "\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "4fCxKSd16i9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "S2mxncXk6lDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Uji Model**"
      ],
      "metadata": {
        "id": "Y6qkTVQQ64IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaEGcONg7AyP",
        "outputId": "70fbfa30-b187-4801-d26e-4a7ff378a80a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "WfjGx2ip8YzW",
        "outputId": "217f8fdc-e9e4-455c-ccf2-1fb6e607bf49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"my_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │          \u001b[38;5;34m16,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                            │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;34m64\u001b[0m,      │       \u001b[38;5;34m3,938,304\u001b[0m │\n",
              "│                                      │ \u001b[38;5;34m1024\u001b[0m))                      │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m66\u001b[0m)               │          \u001b[38;5;34m67,650\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                            │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
              "│                                      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>))                      │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">67,650</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,022,850\u001b[0m (15.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,850</span> (15.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,022,850\u001b[0m (15.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,850</span> (15.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "kO1yJWXr8as9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmHMlCAk8qrG",
        "outputId": "dcadfc7b-2d1e-4e4f-a46a-633de9b30170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([33, 36, 58, 23, 45,  4, 52,  9, 46, 35,  7, 28, 40,  0, 14, 48, 28,\n",
              "       40, 23, 56,  0, 39, 24, 52, 19, 49, 22,  9, 25, 44, 33, 42, 16, 46,\n",
              "       30,  0, 17, 32, 60, 59, 57, 25, 62,  1, 49, 52, 49, 42,  8,  6, 14,\n",
              "       18, 12, 63, 31, 22, 24, 33, 40, 22, 60, 11, 18,  4, 15,  9, 33, 51,\n",
              "       29, 64, 14, 65, 52, 57, 34, 59,  9, 36, 43, 44, 42, 28, 55, 32, 60,\n",
              "       31, 22, 21, 43, 18, 11,  5, 60, 11, 56, 30,  2, 47, 65, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lBTjgq98ryp",
        "outputId": "374d4614-9133-4a42-e950-6f26184603c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"friend,\\nAnd I'll not wish thee to her.\\n\\nPETRUCHIO:\\nSignior Hortensio, 'twixt such friends as we\\nFew \"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"TWsJf$m.gV,Oa[UNK]AiOaJq[UNK]ZKmFjI.LeTcCgQ[UNK]DSutrLw\\njmjc-'AE;xRIKTaIu:E$B.TlPyAzmrUt.WdecOpSuRIHdE:&u:qQ hzM\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Model**"
      ],
      "metadata": {
        "id": "w4QbbSxx8xfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambahan optimizer dan fungsi loss\n"
      ],
      "metadata": {
        "id": "X57kqv6b82EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "UkxBbBhj8u36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBv1skqF86Z6",
        "outputId": "4e9c59a9-670a-4451-ee3e-231e2db9b749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.187941, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1y72k7_89iG",
        "outputId": "c65aaf15-4455-47dd-9db8-d5255f36d3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.88699"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "G17kqTMM9Pl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Konfigurasi Cekpoints"
      ],
      "metadata": {
        "id": "z7Cm31AB9RZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "# Add .weights.h5 to the filename\n",
        "checkpoint_prefix += \".weights.h5\"  # This line is the fix\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "3IPZzF9S9WHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lakukan Proses Training\n"
      ],
      "metadata": {
        "id": "wtrynKsV9ig_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "6GV9hriC9j3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTpEe1279niX",
        "outputId": "b1a34bf2-8da9-40b9-89d0-7583d1073896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m991s\u001b[0m 6s/step - loss: 3.1066\n",
            "Epoch 2/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m972s\u001b[0m 6s/step - loss: 1.9254\n",
            "Epoch 3/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1000s\u001b[0m 6s/step - loss: 1.6340\n",
            "Epoch 4/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1009s\u001b[0m 6s/step - loss: 1.4835\n",
            "Epoch 5/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m981s\u001b[0m 6s/step - loss: 1.3941\n",
            "Epoch 6/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m989s\u001b[0m 6s/step - loss: 1.3330\n",
            "Epoch 7/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m973s\u001b[0m 6s/step - loss: 1.2792\n",
            "Epoch 8/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m984s\u001b[0m 6s/step - loss: 1.2361\n",
            "Epoch 9/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m979s\u001b[0m 6s/step - loss: 1.1957\n",
            "Epoch 10/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m974s\u001b[0m 5s/step - loss: 1.1510\n",
            "Epoch 11/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m989s\u001b[0m 6s/step - loss: 1.1133\n",
            "Epoch 12/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m979s\u001b[0m 6s/step - loss: 1.0702\n",
            "Epoch 13/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m953s\u001b[0m 6s/step - loss: 1.0248\n",
            "Epoch 14/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m981s\u001b[0m 6s/step - loss: 0.9768\n",
            "Epoch 15/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m977s\u001b[0m 5s/step - loss: 0.9279\n",
            "Epoch 16/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m970s\u001b[0m 5s/step - loss: 0.8801\n",
            "Epoch 17/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1005s\u001b[0m 6s/step - loss: 0.8304\n",
            "Epoch 18/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m982s\u001b[0m 6s/step - loss: 0.7793\n",
            "Epoch 19/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m982s\u001b[0m 6s/step - loss: 0.7309\n",
            "Epoch 20/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m958s\u001b[0m 6s/step - loss: 0.6883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generate Teks**"
      ],
      "metadata": {
        "id": "vDWv6oQg9sgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model  # Changed model8 to model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "QdSDcUYy9vqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "zaO5bK9mtAF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B50K76F4tOX4",
        "outputId": "37f3cb68-5ffa-4c67-900a-d5d655545509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "What a foot is that he straight?\n",
            "\n",
            "CATESBY:\n",
            "No, sir, he, and give it your grace's wife:\n",
            "I' the main blatter, entreathed the crebt,\n",
            "Which almost husbands, though thou wast done to thee\n",
            "Than with a perpetual sicketing.\n",
            "\n",
            "MENENIUS:\n",
            "Twice fives on a charm, to be them\n",
            "To coll him here alone: Lewis, if God forslike\n",
            "High banishment, the rest, if this hand soul\n",
            "Near to use me wear an else.\n",
            "\n",
            "PAULINA:\n",
            "Well, be was have:\n",
            "'Tis very short!\n",
            "\n",
            "PARIS:\n",
            "Thy grief is Elentand's pains to her soft,\n",
            "And so I have heard him straight.\n",
            "\n",
            "LUCENTIO:\n",
            "While we weep, so dear as I exproceded,\n",
            "Wherein thy great good corns once as mount?\n",
            "Of vain, thou wretch'd! conveyent me.\n",
            "\n",
            "CAMILLO:\n",
            "Be a think, follow, gentle sorrow!\n",
            "\n",
            "Second Gentleman:\n",
            "This is the lamb. Yet as muny as my remains:\n",
            "Let me in suspicion; for a Clarence would,\n",
            "Like him their lips been kiss'd upon him, sir,\n",
            "I must be green, let Romeo cheers;\n",
            "But now 'tisched by the Earl of Whice to-day.\n",
            "Red this war I hear, the rest resign my good\n",
            "Quietness, though the youth \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.6533467769622803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdLBf1xCtUpr",
        "outputId": "1eaf54c7-d9a7-4165-c2db-72addd9fecc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThis deed is quick hear it.\\n\\nHERMIONE:\\nMenry in all but slipp'd, let's so tender\\nWise men rage, with rooms and the Aufiditees,\\nWas like an oath reward, when he detested?\\n\\nFirst Huntsman:\\nImen these sorrows is simple and their tribble.\\n\\nPETRUCHIO:\\nShould it not hard it?\\n\\nLUCIO:\\n\\nISABELLA:\\nTell near, Gremio, rids unto the tongue.\\n\\nKING RICHARD III:\\nThink you, my noble cousin Hark!\\nThis tooth thy bridies self to visit him,\\nAs an hundred cavernings his action.\\nAn officer, throne; I have been out-day nor need,\\nAnd not believe thee quiet creft his name;\\nAh, so I firm, 'tis better sleeps with which stamps\\nAbove a ballad is repeals, and cries 'God save thee;\\nMuth, I think she will come retiren;\\nAnd often when famous store your master!\\nLorb, Isabella, some woman.\\n\\nLUCIO:\\nI take Henry from his sones, and by the hostess of thy name!\\nBesides, he wounded he will settled.\\n\\nPETRUCHIO:\\nNow comes these are the Antilus, brings it out, as your\\nfail. Preyony Pompet, or else\\nBut fetter you at last I see y\"\n",
            " b\"ROMEO:\\nThen lie she says, if you hid, being given a taw,\\nAnd scarry, perform, for your body proclast!\\n\\nCLIFFORD:\\nHere comes a mellain thrice are now\\nTo use it new-believed, when you make so parte\\nTo take her here of heaven, but lour us on\\nThis is the case and call'd blood flow out an\\nacquittarins therefore, tell him prove\\nTo see your ladyship till nower well prove.\\n\\nTRANIO:\\nNathles, shame! she will not stay.\\n\\nPETRUCHIO:\\nPerpase Thy bron:\\nAway with Clarence to Harry Pedrar's father's head?\\n\\nWARWICK:\\nThen, Camillo, this is set many grey,\\nTell me of home, and presently report\\nHath bring me of the nurse that you might.\\n\\nLEONTES:\\nWhither with shifts receive about my houses:\\nI have been married, not till us his wife i' the way\\nOnce those whose cries nor to say into his master's,\\nUnless he do consume, do not something me;\\nRescue, rise, when traitor rich, whose piest case rather\\nBid him ever long live Duke of Tybalt,\\nUnder left me gree, for that wild liver\\nI have with you as 'ming my heavens to my z\"\n",
            " b\"ROMEO:\\nBy after thoughts, Clarence, when you weary was it about,\\nAnd therefore, if you'ld lose his body, the oak distarg,\\nHave not with riches more to what it,\\nRemaining such eyes as we pass: whose care is thire face\\nOn this condition, to be intermert bore.\\n\\nQUEEN ELIZABETH:\\nThy law of whom it will not stir us? Set it out!\\nA packetor and the princes have cause to weep the\\nseaford.\\nHow need it be, that thought me not, Juliet.\\n\\nCLIFFORD:\\nI am in yurbering Anmeasing,\\nBoth ministers fallen by but comfort.\\n\\nNORTHUMBERLAND:\\nIt is, my lord, awake a lovily\\nAre that profise hath been still thrusteth not, nor so,\\nBecause some name where means to creak aboard;\\nFor she was not infect another's after\\nAnd send the war thou hast made to all afresh.\\nBlown with a noble mark upon your age!\\nThen, Gremio, are us not think'd it is so weak:\\n'Tis this nine the heel, the rest, rote upright\\nVeare our twenty times, here was nothing divine\\nThat makes and rotten parces of their seat.\\n\\nLADY ANNE:\\nSignior Gremio, come; y\"\n",
            " b\"ROMEO:\\nI should am, that did lease your lordship.\\n\\nRICHARD:\\nAre we enough tongue-dance taken yourselves?\\n\\nPARIS:\\nHow well Rutar Grumio: rise, being the wiseless\\nrescuer'd.\\n\\nFirst Senator:\\nFather, come, though I turn mine heart\\nImonged and undertake the absence makes\\nAmbs such a dule by the appearing beed;\\nThis dead devil,--will rest thou had said 'Wheep doth:\\n'Tis most miles before her, Signior Edward Isabell:\\nAnd yet you can, I was no more than tendies\\nTwo mourning wee her by.\\n\\nCATESBY:\\nMy lord, Henry you think with arts\\nOf death thine idle with death-ashifal!\\nOr else by him for a herald rootmen throne,\\nOur town and in joy's earth sad and a love\\nAnd other reasons in his body;\\nThis noble cousin Hastings, whose man along\\nwith him, who art thou so prepared?\\n\\nPETRUCHIO:\\nNay, good my, both; or give me this is Lord:\\n'Tis done me not: anon, as you can.\\nHe trells with flawls. And yet I tell you, fulfsel,\\nIf he were set the dwelling wrongs.\\n\\nYORK:\\nI jointed me to the Place,\\nWhich for my ease above w\"\n",
            " b\"ROMEO:\\nPerhaps, ho! Are gone,\\nTo speed and flouted us with if all my hands\\nshall seem, as I talk of his one hath got\\nThe atternor for a half-for such a gentleman.\\nThis is not so, for by the time use wealth\\nWhich he disdain'd with careful welcome,\\nAnd trainly gentle in roaring thing,\\nDid plague in sumple gar made respecturn;\\nWas sent to their subject sits, call'd you the people,\\nAnd she shall send for, sent him o'er. This is free\\nTo cheer it with shifts with hate, and mendrance\\nTa left phortisement. If any plague oath\\nThat young Henry live upon from me to her.\\n\\nPETRUCHIO:\\nHis nurse is this? Go, get thee quickly should\\nBy holy city, like a discoursedee,\\nNozel and effected and open thou shalt Miscuries\\nOf outward cockerous ancestor, all;\\nMore healths, of whose name his battle from\\nthee, thou hast braved me ruth, shot without biar\\nshe must die in branch nor gazed in peace.\\n\\nMERCUTIO:\\nO, though made thee hang them without it as pales\\nRichard let Edward and Citizen:\\nThe nobles be guilt before the \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 6.240656614303589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ekspor Model Generator**"
      ],
      "metadata": {
        "id": "sK2bFgPStYvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "id": "3c56qj6gtbai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "mMWnGkXWtecU",
        "outputId": "3fc8738e-3e5e-4b24-d2c5-376a7c9d87be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7f68b91d1b29>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ROMEO:'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext_char\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TUGAS**"
      ],
      "metadata": {
        "id": "QymDLhiuxRjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "VtyPgH-6xWm8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}